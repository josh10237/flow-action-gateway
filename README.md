# Wispr Actions

Voice-controlled gateway for productivity apps using the Model Context Protocol (MCP).

## What This Does

Speak natural language commands and execute actions across multiple apps without writing integration code for each one. Built on MCP (Model Context Protocol) to demonstrate O(1) integration scaling.

**Example commands:**
- "list files on desktop"
- "read test.txt"
- "search for pdf files"

## Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run the app:**
   ```bash
   python main.py
   ```

3. **First-time setup:**
   - You'll be prompted for your OpenAI API key
   - Configuration is saved to `.env`

## Controls

- **V** - Hold to record voice (release to process)
- **Q** - Quit

## Architecture Overview

**Voice â†’ Action Pipeline:**
```
Voice Input
    â†“
[Audio Capture] â†’ PyAudio buffers audio while V is held
    â†“
[Whisper API] â†’ Transcribes to text
    â†“
[GPT-4 + MCP Tool Catalog] â†’ Parses intent into structured function call
    â†“
[MCP Gateway] â†’ Routes to correct MCP server
    â†“
[MCP Server] â†’ Executes the action (filesystem, GitHub, etc.)
    â†“
[Terminal UI] â†’ Shows result
```

## Why MCP?

Traditional approach: Write custom integration for each service (Slack, Gmail, Notion, etc.)
- **Problem:** N integrations = N Ã— maintenance cost
- **Problem:** Every API change breaks your code

MCP approach: Connect to standard MCP servers that service providers maintain
- **Benefit:** Add new services via config only (no code changes)
- **Benefit:** Service providers maintain their own MCP servers
- **Benefit:** As MCP ecosystem grows, system gets more powerful automatically

**Adding a new service:**
```json
// Just add to mcp_config.json
{
  "name": "slack",
  "display_name": "Slack",
  "icon": "ðŸ’¬",
  "enabled": true,
  "command": "npx",
  "args": ["-y", "@modelcontextprotocol/server-slack"],
  "env": {
    "SLACK_TOKEN": "${SLACK_TOKEN}"
  }
}
```

That's it. No code changes needed.

## Project Structure

```
flow-action-gateway/
â”œâ”€â”€ main.py                    # Entry point with onboarding
â”œâ”€â”€ mcp_config.json            # MCP server configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gateway/
â”‚   â”‚   â”œâ”€â”€ mcp_gateway.py     # MCP connection manager
â”‚   â”‚   â”œâ”€â”€ mcp_config.py      # Config loader
â”‚   â”‚   â””â”€â”€ intent_parser.py   # GPT-4 intent understanding
â”‚   â”œâ”€â”€ voice/
â”‚   â”‚   â”œâ”€â”€ capture.py         # Audio recording
â”‚   â”‚   â””â”€â”€ transcription.py   # Whisper integration
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app.py             # Textual terminal UI
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROMPT.md              # Interview challenge prompt
â”‚   â”œâ”€â”€ ABSTRACT.md            # Architecture rationale
â”‚   â””â”€â”€ ARCHITECTURE.md        # Technical deep dive
â””â”€â”€ requirements.txt
```

## Key Design Decisions

**Terminal UI instead of web/native app:**
- Focus on demonstrating the MCP gateway architecture
- Same backend can power any frontend later
- Faster to build, easier to demo core functionality

**OpenAI Whisper + GPT-4:**
- Single provider (simpler setup)
- GPT-4 function calling is excellent for structured outputs
- Proven reliability

**MCP Gateway Pattern:**
- O(1) integration effort per service (vs O(N) for custom APIs)
- Leverage community-built MCP servers
- Future-proof as ecosystem grows

## Current Limitations

- **Latency:** ~4-6 seconds per command (Whisper API + GPT-4 API calls)
  - Could be improved with local Whisper (faster-whisper) + GPT-4o-mini
- **UI:** Basic terminal interface (sufficient for demo)
- **Services:** Currently filesystem + GitHub (more can be added via config)

## Development

Built for the Wispr full-stack engineering challenge. Demonstrates:
- Scalable architecture (MCP gateway pattern)
- LLM integration (GPT-4 function calling for intent parsing)
- Voice processing pipeline (Whisper â†’ GPT-4 â†’ MCP â†’ Action)
- Clean abstractions and separation of concerns
