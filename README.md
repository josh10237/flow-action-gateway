# Wispr Actions

Voice-controlled gateway that combines **fast voice input** with **rich visual output** for productivity apps.

## What This Does

Speak natural language commands â†’ Get beautiful, structured visual results. No custom integration code needed for each app.

**Why voice + visual?**
- **Voice input**: Fastest way to communicate (as fast as you can think)
- **Visual output**: Most efficient way to comprehend information (layouts > text > audio)

**Example commands:**
- "list files on desktop" â†’ Visual file browser with icons and metadata
- "search github for react repos" â†’ Cards showing repos with stars, language, and clickable links
- "search for AI news" â†’ Formatted search results with titles, descriptions, and sources

## Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run the app:**
   ```bash
   python main.py
   ```

3. **First-time setup:**
   - You'll be prompted for your OpenAI API key
   - Configuration is saved to `.env`

## Controls

- **V** - Hold to record voice (release to process)
- **Q** - Quit

## Architecture Overview

This system solves two fundamental challenges:

### Challenge 1: Application Integration at Scale â†’ MCP Gateway

**Traditional approach fails:** Building custom integrations for thousands of apps (Linear, Notion, GitHub, Slack, etc.) = N Ã— maintenance cost. Every API change breaks your code.

**MCP Gateway solution:** One gateway that routes to app-maintained MCP servers. O(1) integration effort. As MCP ecosystem grows, system automatically gets more powerfulâ€”no code changes needed.

### Challenge 2: Information Display at Scale â†’ Component Library + Data Bindings

**Traditional approach fails:** Building custom UI for every function in every app = NÃ—M maintenance nightmare.

**Component Library solution:** Reusable UI primitives (cards, lists, key-value pairs, links) cover 80%+ of use cases. Declarative data bindings map MCP responses â†’ UI components. Future: LLM-assisted binding generation with cachingâ€”once ANY user executes a function, we generate the optimal UI for all future users.

**Complete Pipeline:**
```
Voice Input (fast communication)
    â†“
[Audio Capture] â†’ PyAudio buffers audio while V is held
    â†“
[Whisper API] â†’ Transcribes to text
    â†“
[GPT-4 + MCP Tool Catalog] â†’ Parses intent into structured function call
    â†“
[MCP Gateway] â†’ Routes to correct MCP server
    â†“
[MCP Server] â†’ Executes the action (filesystem, GitHub, etc.)
    â†“
[Data Binding Router] â†’ Maps response â†’ UI components
    â†“
[Terminal UI] â†’ Rich visual output (efficient comprehension)
```

## Why This Architecture?

**Input Side: MCP Gateway for O(1) Scaling**
- **Problem:** ~200 apps with 100M+ users, plus long tail of niche tools. Covering 80% of workflows = thousands of custom integrations.
- **Solution:** ONE gateway that dynamically routes to MCP servers maintained by app providers themselves.
- **Result:** Add new services via config only. Zero maintenance burden. Automatic ecosystem benefits.

**Output Side: Component Library for O(1) Scaling**
- **Problem:** Voice input is fastest (as fast as thinking), but visual comprehension is most efficient. Millions of engineers work on UI/UX for a reasonâ€”layouts, components, and styling communicate nuanced information better than text or audio.
- **Solution:** Reusable component library + declarative data bindings. Post-process with LLM to generate bindings, cache for all users.
- **Result:** Build UI components once, map any function to them. Network effectsâ€”first execution generates binding for everyone.

**Adding a new service:**
```json
// Just add to mcp_config.json
{
  "name": "slack",
  "display_name": "Slack",
  "icon": "ðŸ’¬",
  "enabled": true,
  "command": "npx",
  "args": ["-y", "@modelcontextprotocol/server-slack"],
  "env": {
    "SLACK_TOKEN": "${SLACK_TOKEN}"
  }
}
```

That's it. No code changes needed.

## Project Structure

```
flow-action-gateway/
â”œâ”€â”€ main.py                    # Entry point with onboarding
â”œâ”€â”€ mcp_config.json            # MCP server configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gateway/
â”‚   â”‚   â”œâ”€â”€ mcp_gateway.py     # MCP connection manager
â”‚   â”‚   â”œâ”€â”€ mcp_config.py      # Config loader
â”‚   â”‚   â””â”€â”€ intent_parser.py   # GPT-4 intent understanding
â”‚   â”œâ”€â”€ voice/
â”‚   â”‚   â”œâ”€â”€ capture.py         # Audio recording
â”‚   â”‚   â””â”€â”€ transcription.py   # Whisper integration
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ app.py             # Textual terminal UI
â”‚       â””â”€â”€ components/        # Reusable UI components
â”‚           â”œâ”€â”€ github.py      # GitHub data bindings
â”‚           â””â”€â”€ search.py      # Search results data bindings
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROMPT.md              # Interview challenge prompt
â”‚   â”œâ”€â”€ ABSTRACT.md            # Architecture rationale
â”‚   â””â”€â”€ ARCHITECTURE.md        # Technical deep dive
â””â”€â”€ requirements.txt
```

## Key Design Decisions

**Dual O(1) Scaling Strategy:**
- **Input:** MCP Gateway handles application integration without custom code per app
- **Output:** Component Library + Data Bindings handle UI without custom components per function
- Both sides scale independently as ecosystem grows

**Terminal UI instead of web/native app:**
- Focus on demonstrating core architecture (MCP gateway + component bindings)
- Same backend can power any frontend later
- Faster to build, easier to demo dual-scaling concept

**OpenAI Whisper + GPT-4:**
- Single provider (simpler setup)
- GPT-4 function calling is excellent for structured outputs
- Proven reliability for voice â†’ structured intent

**Component-Based Output:**
- Reusable UI primitives (cards, lists, key-value displays)
- Declarative data bindings separate from component logic
- Easy to extendâ€”add new binding without touching component code

## Current Limitations

- **Latency:** ~2.6-5.5 seconds per command
  - Whisper API: 1.5-3s (network RTT)
  - GPT-4 Intent Parsing: 1-2s (network RTT)
  - MCP Execution: 0.1-0.5s (local IPC)
  - Sequential bottleneck: Cannot parallelize ASR â†’ Intent â†’ Execution
  - Potential improvements: Intent caching, tool filtering, faster models (GPT-4o-mini)
- **UI Components:** Basic terminal components (cards, lists, key-value pairs)
  - Sufficient for demo, can be extended to web/native later
- **Data Bindings:** Manual for now (GitHub, Brave Search)
  - Future: LLM-assisted binding generation with caching
- **Services:** Currently filesystem, GitHub, Brave Search
  - More can be added via config only (no code changes)

## Testing

Run the test suite:
```bash
cd tests && python run_all_tests.py
```

**Test structure:**
- **Unit tests**: Individual functions (HTML processing, field filtering)
- **Functional tests**: Full application flows (data binding, APIs, voice pipeline)

Tests automatically skip components that aren't available (e.g., API keys, audio drivers).

## Development

Built for the Wispr full-stack engineering challenge. Demonstrates:
- **Dual O(1) Scaling:** MCP Gateway (input) + Component Library (output)
- **Voice â†’ Visual Pipeline:** Fast input meets efficient comprehension
- **LLM Integration:** GPT-4 function calling for intent parsing
- **Declarative UI:** Data bindings separate from component logic
- **Clean Abstractions:** Modular architecture for easy extension
